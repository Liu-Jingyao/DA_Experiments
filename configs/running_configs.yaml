# global params

environment: 'beijing' # vpn, local, quanzhou, neimeng, beijing
batch_size: 32 # 32
map_batch_size: 1000
eval_steps: 200 # 1000/200
logging_steps: 100 # 100/100
ignore_cache: true
repeat_num: 3

---
# single test config

default:
  dataset: 'rotten_tomatoes' # imdb, rotten_tomatoes, yelp-5, amazon-5
  model: 'lstm' # cnn, distilbert, lstm
  augmentations: ['pred_loss_replacement'] # keyword_enhance, random_word_dropout, tfidf_word_dropout, synonym_replacement,
                                        # random_deletion, random_swap, random_insertion,
                                        # hidden_state_pooling, hidden_state_cnn, online_random_replacement, pred_loss_replacement
  aug_params: [0.5] #0.005, dont remain blank, 0.5
  epochs: 10

---
# workflow test config
training_config:
  distilbert:
      imdb:
          epochs: 3
      rotten_tomatoes:
          epochs: 3
      amazon-5:
          epochs: 1
      yelp-5:
          epochs: 1
  cnn:
      imdb:
          epochs: 20
      rotten_tomatoes:
          epochs: 20
      amazon-5:
          epochs: 1
      yelp-5:
          epochs: 1
  lstm:
      imdb:
          epochs: 20
      rotten_tomatoes:
          epochs: 20
      amazon-5:
          epochs: 1
      yelp-5:
          epochs: 1
workflow_config:
  baseline:
    models: [ distilbert, cnn, lstm ]
    datasets: [ imdb, rotten_tomatoes, amazon-5, yelp-5 ]
  random_word_dropout:
    models: [ distilbert, cnn, lstm ]
    datasets: [ imdb, rotten_tomatoes, amazon-5, yelp-5 ]
    prob: 0.005
#  tfidf_word_dropout:
#    models: [ distilbert, cnn, lstm ]
#    datasets: [ imdb, rotten_tomatoes, amazon-5, yelp-5 ]
#    prob: 0.005
#  synonym_replacement:
#    models: [ distilbert, cnn, lstm ]
#    datasets: [ imdb, rotten_tomatoes, amazon-5, yelp-5 ]
#    prob: 0.25
#  online_random_replacement:
#    models: [ distilbert, lstm ]
#    datasets: [ imdb, rotten_tomatoes, amazon-5 ]
#    prob: 0.25
#  loss_based_replacement:
#    models: [ distilbert, lstm ]
#    datasets: [ imdb, rotten_tomatoes, amazon-5 ]
#    prob: 0.25
#  pred_loss_replacement:
#    models: [ distilbert, lstm ]
#    datasets: [ imdb, rotten_tomatoes, amazon-5 ]
#    prob: 0.25