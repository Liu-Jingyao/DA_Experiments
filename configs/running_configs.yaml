# global params

environment: 'vpn' # vpn, local, quanzhou, neimeng, beijing
batch_size: 32 # 32
map_batch_size: 1000
eval_steps: 200 # 1000/200
logging_steps: 100 # 100/100
ignore_cache: true
repeat_num: 3

---
# single test config

default:
  dataset: 'rotten_tomatoes' # imdb, rotten_tomatoes, yelp-5, amazon-5
  model: 'lstm' # cnn, distilbert, lstm
  augmentations: ['pred_loss_replacement'] # keyword_enhance, random_word_dropout, tfidf_word_dropout, synonym_replacement,
                                        # random_deletion, random_swap, random_insertion,
                                        # hidden_state_pooling, hidden_state_cnn, online_random_replacement, pred_loss_replacement
  aug_params: [0.5] #0.005, dont remain blank, 0.5
  epochs: 10

---
# workflow test config
training_config:
  distilbert:
    imdb:
        epochs: 2
    sst-2:
        epochs: 2
    rotten_tomatoes:
        epochs: 2
    amazon-5:
        epochs: 2
    yelp-5:
        epochs: 2
  roberta:
    imdb:
        epochs: 2
        batch_size: 4
    sst-2:
        epochs: 2
    rotten_tomatoes:
        epochs: 2
    amazon-5:
        epochs: 2
        batch_size: 16
    yelp-5:
        epochs: 2
        batch_size: 16
  albert:
    imdb:
        epochs: 2
        batch_size: 4
    sst-2:
        epochs: 2
    rotten_tomatoes:
        epochs: 2
    amazon-5:
        epochs: 2
        batch_size: 16
    yelp-5:
        epochs: 2
        batch_size: 16
  xlnet:
    imdb:
        epochs: 2
        batch_size: 2
    sst-2:
        epochs: 2
    rotten_tomatoes:
        epochs: 2
    amazon-5:
        epochs: 2
        batch_size: 2
    yelp-5:
        epochs: 2
        batch_size: 2
  electra:
    imdb:
        epochs: 2
        batch_size: 8
    sst-2:
        epochs: 2
    rotten_tomatoes:
        epochs: 2
    amazon-5:
        epochs: 2
        batch_size: 8
    yelp-5:
        epochs: 2
        batch_size: 8
workflow_config:
#  baseline:
#    models: [ distilbert ]
#    datasets: [ yelp-5 ]
#    #    p: [0.05]
#    train_size: [ 25000 ]

#  baseline:
#    models: [ distilbert ]
#    datasets: [ imdb ]
##    p: [0.05]
#    train_size: [ 700 ]
  #  random_word_dropout:
  #    models: [ distilbert ]
  #    datasets: [ amazon-5 ]
  #    p: [0.05]
  #    train_size: [ 500 ]
  #    n_aug: 4
#  random_deletion:
#    models: [ distilbert ]
#    datasets: [ rotten_tomatoes, imdb, yelp-5, amazon-5 ]
#    p: [ 0.1, 0.2, 0.3, 0.5 ]
#    train_size: [ 500 ]
#    n_aug: 4
#  tfidf_word_dropout:
#    models: [ distilbert ]
#    datasets: [ rotten_tomatoes, imdb, yelp-5, amazon-5 ]
#    p: [ 0.1, 0.2, 0.3, 0.5 ]
#    train_size: [ 500 ]
#    n_aug: 4
#  random_deletion:
#    models: [ distilbert ]
#    datasets: [ amazon-5 ]
#    p: [0.05]
#    train_size: [ 300, 500, 700 ]
#  tfidf_word_dropout:
#    models: [ distilbert ]
#    datasets: [ amazon-5 ]
#    p: [0.05]
#    train_size: [ 300, 500, 700 ]
  synonym_replacement:
    models: [ distilbert ]
    datasets: [ rotten_tomatoes, amazon-5 ]
    p: [ 0.05 ]
    train_size: [ 100 ]
    n_aug: [ 1,2,4,8,16,32 ]
  pred_loss_replacement:
    models: [ distilbert ]
    datasets: [ rotten_tomatoes, amazon-5 ]
    p: [ 0.05 ]
    train_size: [ 100 ]
    n_aug: [ 1,2,4,8,16,32 ]

#  synonym_replacement:
#    models: [ distilbert ]
#    datasets: [ yelp-5 ]
#    prob: 0.05
#  pred_loss_replacement:
#    models: [ distilbert ]
#    datasets: [ yelp-5 ]
#    prob: 0.05